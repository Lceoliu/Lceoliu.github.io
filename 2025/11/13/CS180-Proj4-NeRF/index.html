

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/lc.jpg">
  <link rel="icon" href="/img/lc.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Chang Leo">
  <meta name="keywords" content="">
  
    <meta name="description" content="CS180 project4: NeRF Theoretical Background What is NeRF(Neural Radiance Fields)? NeRF (proposed in the original 2020 paper) is the technique to represent a 3D scene volumetrically (i.e., without">
<meta property="og:type" content="article">
<meta property="og:title" content="CS180-Proj4-NeRF">
<meta property="og:url" content="https://lceoliu.github.io/2025/11/13/CS180-Proj4-NeRF/index.html">
<meta property="og:site_name" content="Chang&#39;s Blog">
<meta property="og:description" content="CS180 project4: NeRF Theoretical Background What is NeRF(Neural Radiance Fields)? NeRF (proposed in the original 2020 paper) is the technique to represent a 3D scene volumetrically (i.e., without">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.it-jim.com/wp-content/uploads/2023/05/1-1-768x223.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_no_pe.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png">
<meta property="og:image" content="https://www.it-jim.com/wp-content/uploads/2023/05/3d-object-representation.png">
<meta property="og:image" content="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-jelly.png">
<meta property="og:image" content="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-scene-representation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_selfie.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180grid.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve_selfie1.png">
<meta property="og:image" content="https://cal-cs180.github.io/fa25/hw/proj4/assets/mlp_nerf.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image1000.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image3000.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image5000.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola300.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola1500.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola5000.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_log.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola_psnr.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1808192_lego-ezgif.com-loop-count.gif">
<meta property="og:image" content="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola-ezgif.com-loop-count.gif">
<meta property="article:published_time" content="2025-11-13T23:09:41.000Z">
<meta property="article:modified_time" content="2025-11-14T00:18:45.548Z">
<meta property="article:author" content="Chang Leo">
<meta property="article:tag" content="notes">
<meta property="article:tag" content="CS180">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="NeRF">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://www.it-jim.com/wp-content/uploads/2023/05/1-1-768x223.png">
  
  
  
  <title>CS180-Proj4-NeRF - Chang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lceoliu.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"9HyUa6m2LMnz9pU9dVwuqyEV-gzGzoHsz","app_key":"LHEhgO1t44IynNOlOy97ZJT1","server_url":null,"path":"window.location.pathname","ignore_local":true},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/node-tikzjax@latest/css/fonts.css" /><style>.tikzjax { display: block; text-align: center; user-select: none; }</style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav
  id="navbar"
  class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"
>
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Chang</strong>
    </a>

    <button
      id="navbar-toggler-btn"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
           
        <li class="nav-item">
          <a class="nav-link" href="/">
            <i class="iconfont icon-home-fill"></i> 首页</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
        </li>
           
        <li class="nav-item">
          <a class="nav-link" href="/archives/">
            <i class="iconfont icon-archive-fill"></i> 归档</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
        </li>
           
        <li class="nav-item">
          <a class="nav-link" href="/categories/">
            <i class="iconfont icon-category-fill"></i> 分类</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
          <ul class="sub-menu">
            <!--//遍历出二级菜单（就是竖着的那部分）-->
            
            <li>
              <!--//a标签里当然是输出二级菜单的路径咯-->
              <a href="/categories/Blogs/">
                <!--//i标签里输出二级菜单的icon的class咯-->
                <i class="iconfont icon-category-fill"></i>
                <!--//这里输出二级菜单名咯-->
                Blogs
              </a>
            </li>
            
            <li>
              <!--//a标签里当然是输出二级菜单的路径咯-->
              <a href="/categories/Papers/">
                <!--//i标签里输出二级菜单的icon的class咯-->
                <i class="iconfont icon-category-fill"></i>
                <!--//这里输出二级菜单名咯-->
                Papers
              </a>
            </li>
            
            <li>
              <!--//a标签里当然是输出二级菜单的路径咯-->
              <a href="/categories/Game%20Devs/">
                <!--//i标签里输出二级菜单的icon的class咯-->
                <i class="iconfont icon-category-fill"></i>
                <!--//这里输出二级菜单名咯-->
                Game Devs
              </a>
            </li>
            
          </ul>
          
        </li>
           
        <li class="nav-item">
          <a class="nav-link" href="/tags/">
            <i class="iconfont icon-tags-fill"></i> 标签</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
        </li>
           
        <li class="nav-item">
          <a class="nav-link" href="/about/">
            <i class="iconfont icon-user-fill"></i> 关于</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
        </li>
           
        <li class="nav-item">
          <a class="nav-link" href="/links/">
            <i class="iconfont icon-link-fill"></i> 友链</a
          >
          <!--以上的部分是一级导航的部分-->
          <!--这里判断有没有二级菜单，有的话遍历出二级菜单（就是竖着的那部分）-->
          
        </li>
         
        <li class="nav-item" id="search-btn">
          <a
            class="nav-link"
            target="_self"
            href="javascript:;"
            data-toggle="modal"
            data-target="#modalSearch"
            aria-label="Search"
          >
            <i class="iconfont icon-search"></i>
          </a>
        </li>
          
        <li class="nav-item" id="color-toggle-btn">
          <a
            class="nav-link"
            target="_self"
            href="javascript:;"
            aria-label="Color Toggle"
          >
            <i class="iconfont icon-dark" id="color-toggle-icon"></i>
          </a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CS180-Proj4-NeRF"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-13 15:09" pubdate>
          2025年11月13日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          434 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          4 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CS180-Proj4-NeRF</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="cs180-project4-nerf">CS180 project4: NeRF</h1>
<h2 id="theoretical-background">Theoretical Background</h2>
<h3 id="what-is-nerfneural-radiance-fields">What is NeRF(Neural Radiance
Fields)?</h3>
<p>NeRF (proposed in the original 2020 paper) is the technique to
represent a 3D scene volumetrically (i.e., without any surfaces) as a
function parametrized by a neural network to render 2D views of such a
scene and to train the network on a set 2D views.</p>
<p>Some reference tutorials about NeRF:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://sites.google.com/berkeley.edu/nerf-tutorial/home">ECCV
2022 NeRF tutorial</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.00379">NeRF review
article</a></p></li>
</ul>
<h3 id="functional-representation-in-2d">Functional Representation in
2D</h3>
<p>How to represent a 2D image on a computer? There are several
ways:</p>
<p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/1-1-768x223.png" srcset="/img/loading.gif" lazyload alt="Functional Representation in 2D"> Representation of 2D images: a)
– pixels , b) – vector, c) – point cloud, d) – functional</p>
<p>Is this all? No, there are more ways to represent an image
mathematically. Let’s look at functional representations (sometimes also
called implicit). There are several ways to use mathematical functions.
First, we can parametrize the color C of a point <span class="math inline">\((x, y)\)</span> as a mathematical function <span class="math inline">\(C = f(x, y)\)</span>. This is a volumetric
representation for a 2D volume; it does not deal with any lines or
curves (which are surfaces in 2D). On the other hand, we can have a
surface representation <span class="math inline">\(f(x,y) = 0\)</span>,
a contour parametrized by an implicit function.</p>
<p>But how can we represent a complicated nonlinear function <span class="math inline">\(f(x, y)\)</span> on the computer? In 2025 we all
know the answer: deep neural networks. There is an experiment that
probably every person really interested in deep learning has tried at
least once: approximate the function <span class="math inline">\(C=f(x,
y)\)</span> with a fully-connected neural network (also known as a
multi-layer perceptron or MLP) and train it on all pixels of an image.
The dataset here consists of tuples <span class="math inline">\((x, y,
C)\)</span> for all image pixels of a single image. Once trained, we use
this MLP to predict the color C for all pixels <span class="math inline">\((x, y)\)</span>, and thus we use <span class="math inline">\(f(x, y)\)</span> to render an image. <strong>This
part is exactly what we will implement in Part 1.</strong> But let's see
how this naive approach works:</p>
<p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_no_pe.png" srcset="/img/loading.gif" lazyload></p>
<p>(From left to right illustrates the network output at 0, 100, 300,
800, 1500 and 3000 training iterations)</p>
<p>It’s not particularly good, despite the neural network having more
parameters than pixels in the image, why? This representation has two
problems:</p>
<ol type="1">
<li><p>The raw coordinates <span class="math inline">\((x, y)\)</span>
are not a good input representation for the neural network. The network
cannot easily learn high-frequency functions from such inputs. So the
solution is to use <strong>positional encoding</strong> (Fourier
features) to map the input coordinates <span class="math inline">\((x,
y)\)</span> to a higher-dimensional space with more frequency
components. <strong>This is what we will implement in Part
1.</strong></p></li>
<li><p>The ReLU-based MLPs can only represent piecewise linear
functions. More sophisticated activation functions (e.g. Sigmoid, Sine)
or network architectures (e.g. SIREN, Fourier Neural Operator) can help
to represent high-frequency functions better.</p></li>
</ol>
<p>This is the better result after applying positional encoding and
sigmoid activation:</p>
<p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="functional-representation-in-3d-tsdf-and-nerf">Functional
Representation in 3D: TSDF and NeRF</h3>
<p>How to represent 3D objects digitally? 3D representations follow the
same ideas as 2D ones.</p>
<p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/3d-object-representation.png" srcset="/img/loading.gif" lazyload></p>
<p>Pixels in 3D become voxels. Point cloud in 3D is defined just like in
2D. Polygonal meshes can be viewed as a special case of vector graphics.
What about the functional representation? Once again, we have two types
of it: surface and volumetric.</p>
<p>Surface functional representation is about describing the surfaces
with the implicit equation <span class="math inline">\(f(x, y, z) =
0\)</span>. This family of methods is called the (truncated) signed
distance function or (T)SDF. The volumetric representation is given by
the formula <span class="math inline">\(C=f(x, y, z)\)</span>, giving
the color <span class="math inline">\(C\)</span> of each 3D point <span class="math inline">\((x, y, z)\)</span>. We can think of these as
“continuous voxels” or 3D translucent object made of colored jelly.</p>
<figure>
<img src="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-jelly.png" srcset="/img/loading.gif" lazyload alt="NeRF as a translucent jelly">
<figcaption aria-hidden="true">NeRF as a translucent jelly</figcaption>
</figure>
<p>This is basically what NeRF is, although in order to achieve better
results, the actual NeRF adds two things: directional dependence and
density.</p>
<h3 id="nerf-theory">NeRF Theory</h3>
<p>There are three main components of NeRF: scene representation,
renderer and the training regime. NeRF represents a 3D scene as a 5D
function parametrized by a neural network:</p>
<p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-scene-representation.png" srcset="/img/loading.gif" lazyload></p>
<p>The inputs are the coordinates <span class="math inline">\(r=(x, y,
z)\)</span> and the viewing direction <span class="math inline">\((\theta, \phi)\)</span>, often replaced by a unit
direction vector <span class="math inline">\(d=(d_1, d_2, d_3)\)</span>.
The actual inputs to the MLP are the positional encodings of these two
vectors. The output is the color <span class="math inline">\(C=(r, g,
b)\)</span> and the density <span class="math inline">\(\sigma\)</span>.</p>
<p>But what about the lighting? The “standard” NeRF makes the following
strict assumptions about the lighting:</p>
<ul>
<li>every point in the 3D scene emits light equally in all directions
(i.e., no specular reflections);</li>
<li>every point in the 3D scene absorbs light according to its density
<span class="math inline">\(\Sigma\)</span> only (i.e., no subsurface
scattering).</li>
</ul>
<p>As a result, the lighting conditions of the scene are frozen and
cannot be changed after training.</p>
<h3 id="differentiable-volume-rendering">Differentiable Volume
Rendering</h3>
<p>As we cannot perceive a 3D scene directly, what we typically want is
to render it from a certain viewpoint or view, specified by the camera
parameters: intrinsic (focal length, image size) and extrinsic (camera
position and direction). The result is a 2D image. Each camera pixel
becomes a ray in the 3D scene. The pixel color includes contributions
from all points along the ray given by the sum (or rather integral, as
our model is continuous) over the points along the ray:</p>
<p><span class="math display">\[
C(r) = \int_{t_n}^{t_f} T(t) \sigma(r(t)) c(r(t), d) dt,
\]</span></p>
<p>where <span class="math inline">\(r(t) = o + t d\)</span> is the
point along the ray at distance <span class="math inline">\(t\)</span>
from the camera center <span class="math inline">\(o\)</span> in
direction <span class="math inline">\(d\)</span>, <span class="math inline">\(c(r(t), d)\)</span> is the color at point <span class="math inline">\(r(t)\)</span> in direction <span class="math inline">\(d\)</span>, <span class="math inline">\(\sigma(r(t))\)</span> is the density at point
<span class="math inline">\(r(t)\)</span>, and <span class="math inline">\(T(t) = \exp(-\int_{t_n}^{t} \sigma(r(s))
ds)\)</span> is the accumulated transmittance from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span>. <span class="math inline">\(T(t)\)</span> gives the fraction of the light
intensity from the point t reaching the camera (the rest is absorbed).
In the rendering slang it is also called “probability of the ray
reaching the point t uninterrupted”.</p>
<p>In practice, NeRF uses a set of discrete points along the ray. For
each point, get the <span class="math inline">\(c\)</span> and <span class="math inline">\(\sigma\)</span> from <span class="math inline">\(f(r(t), d)\)</span>, and then use sum to
approximate the integral. However this would lead to a fixed set of 3D
points, which could potentially lead to overfitting when we train the
NeRF later on. On top of this, we want to introduce some small
perturbation to the points only during training, so that every location
along the ray would be touched upon during training. this can be
achieved by something like
<code>t = t + (np.random.rand(t.shape) * t_width)</code> where
<code>t</code> is set to be the start of each interval.</p>
<h2 id="deliverables">Deliverables</h2>
<ol start="0" type="1">
<li><p>Camera Calibration and 3D Scanning</p>
<ul>
<li>3 screenshots of your camera frustums visualization in Viser:</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_1.png" srcset="/img/loading.gif" lazyload>
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_2.png" srcset="/img/loading.gif" lazyload>
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_3.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Fit a Neural Field to a 2D Image</p>
<ul>
<li><p>Model architecture report (number of layers, width, learning
rate, and other important details)
<code>"num_freqs": 10, "hidden_dim": 256, "num_layers": 3, "learning_rate": 0.01, "iterations": 3000, "batch_size": 10000,</code></p></li>
<li><p>Training progression visualization on both the provided test
image and one of your own images</p></li>
</ul>
<p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png" srcset="/img/loading.gif" lazyload>
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_selfie.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><p>Final results for 2 choices of max positional encoding frequency
and 2 choices of width (2x2 grid) <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180grid.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>PSNR curve for training on one image of your choice</p>
<ul>
<li>on the provided test image <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve.png" srcset="/img/loading.gif" lazyload></li>
<li>on my own image <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve_selfie1.png" srcset="/img/loading.gif" lazyload></li>
</ul></li>
</ul></li>
<li><p>Fit a Neural Radiance Field from Multi-view Images</p>
<ul>
<li><p>Implementation details (Pseudocodes)</p>
<ul>
<li>Create Rays from Cameras</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">transform</span>(<span class="hljs-params">c2w, x_c</span>):<br>    <span class="hljs-comment"># to transform camera coordinates to world coordinates</span><br>    x_w = c2w @ x_c<br>    <span class="hljs-keyword">return</span> x_w<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pixel_to_camera</span>(<span class="hljs-params">K, uv, s</span>):<br>    <span class="hljs-comment"># to transform pixel coordinates to camera coordinates</span><br>    <span class="hljs-comment"># K: intrinsic matrix, uv: pixel coordinates, s: depth</span><br>    fx, fy, cx, cy = K[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], K[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], K[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>], K[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>    u,v = uv[:,<span class="hljs-number">0</span>], uv[:,<span class="hljs-number">1</span>]<br>    x = (u - cx) * s / fx<br>    y = (v - cy) * s / fy<br>    z = s * np.ones_like(u)<br>    <span class="hljs-keyword">return</span> np.stack([x, y, z], axis=-<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pixel_to_ray</span>(<span class="hljs-params">K, c2w, uv</span>):<br>   <span class="hljs-comment"># Convert pixel coordinates to a ray (origin + normalized direction) in world space.</span><br>   point_camera = pixel_to_camera(K, uv, s=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># assume depth s=1.0</span><br>   point_world = transform(c2w, point_camera.T).T<br>   ray_origin = point_world<br>   ray_direction = point_world - c2w[:<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]<br>   ray_direction /= np.linalg.norm(ray_direction) <span class="hljs-comment"># normalize</span><br>   <span class="hljs-keyword">return</span> ray_origin, ray_direction<br></code></pre></td></tr></table></figure>
<ul>
<li>NeRF Network Structure</li>
</ul>
<p><img src="https://cal-cs180.github.io/fa25/hw/proj4/assets/mlp_nerf.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Rays Visualization <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray1.png" srcset="/img/loading.gif" lazyload>
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray2.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Training progression visualization (rendered images at different
training iterations)</p>
<table>

<thead>
<tr>
<th>Iteration</th>
<th>Dataset</th>
<th>Rendered Image</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>Lego</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image1000.png" srcset="/img/loading.gif" lazyload></td>
</tr>
<tr>
<td>3000</td>
<td>Lego</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image3000.png" srcset="/img/loading.gif" lazyload></td>
</tr>
<tr>
<td>5000</td>
<td>Lego</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image5000.png" srcset="/img/loading.gif" lazyload></td>
</tr>
<tr>
<td>300</td>
<td>My Data</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola300.png" srcset="/img/loading.gif" lazyload></td>
</tr>
<tr>
<td>1500</td>
<td>My Data</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola1500.png" srcset="/img/loading.gif" lazyload></td>
</tr>
<tr>
<td>5000</td>
<td>My Data</td>
<td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola5000.png" srcset="/img/loading.gif" lazyload></td>
</tr>
</tbody>
</table></li>
<li><p>PSNR and Loss curves during training</p>
<ul>
<li><p>on the provided lego dataset <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_log.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>on my own dataset <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola_psnr.png" srcset="/img/loading.gif" lazyload></p></li>
</ul></li>
<li><p>Final rendered GIFs</p>
<ul>
<li><p>on the provided lego dataset (batch = 8192, iterations = 5000)
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1808192_lego-ezgif.com-loop-count.gif" srcset="/img/loading.gif" lazyload></p></li>
<li><p>on my own dataset (batch = 8192, sample =128, iterations = 5000)
<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola-ezgif.com-loop-count.gif" srcset="/img/loading.gif" lazyload></p></li>
</ul></li>
</ul></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/notes/" class="print-no-link">#notes</a>
      
        <a href="/tags/CS180/" class="print-no-link">#CS180</a>
      
        <a href="/tags/CV/" class="print-no-link">#CV</a>
      
        <a href="/tags/NeRF/" class="print-no-link">#NeRF</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CS180-Proj4-NeRF</div>
      <div>https://lceoliu.github.io/2025/11/13/CS180-Proj4-NeRF/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Chang Leo</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月13日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/11/24/CS180-Proj5-Diffusion/" title="CS180-Proj5-Diffusion">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CS180-Proj5-Diffusion</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/10/08/CS180-Proj3A/index/" title="CS180-Proj3A">
                        <span class="hidden-mobile">CS180-Proj3A</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"9HyUa6m2LMnz9pU9dVwuqyEV-gzGzoHsz","appKey":"LHEhgO1t44IynNOlOy97ZJT1","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/Lceoliu" target="_blank" rel="nofollow noopener"><span>My Github</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
