<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CS180-Proj4-NeRF</title>
    <link href="/2025/11/13/CS180-Proj4-NeRF/"/>
    <url>/2025/11/13/CS180-Proj4-NeRF/</url>
    
    <content type="html"><![CDATA[<h1 id="cs180-project4-nerf">CS180 project4: NeRF</h1><h2 id="theoretical-background">Theoretical Background</h2><h3 id="what-is-nerfneural-radiance-fields">What is NeRF(Neural RadianceFields)?</h3><p>NeRF (proposed in the original 2020 paper) is the technique torepresent a 3D scene volumetrically (i.e., without any surfaces) as afunction parametrized by a neural network to render 2D views of such ascene and to train the network on a set 2D views.</p><p>Some reference tutorials about NeRF:</p><ul><li><p><a href="https://sites.google.com/berkeley.edu/nerf-tutorial/home">ECCV2022 NeRF tutorial</a></p></li><li><p><a href="https://arxiv.org/abs/2210.00379">NeRF reviewarticle</a></p></li></ul><h3 id="functional-representation-in-2d">Functional Representation in2D</h3><p>How to represent a 2D image on a computer? There are severalways:</p><p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/1-1-768x223.png" alt="Functional Representation in 2D"> Representation of 2D images: a)– pixels , b) – vector, c) – point cloud, d) – functional</p><p>Is this all? No, there are more ways to represent an imagemathematically. Let’s look at functional representations (sometimes alsocalled implicit). There are several ways to use mathematical functions.First, we can parametrize the color C of a point <span class="math inline">\((x, y)\)</span> as a mathematical function <span class="math inline">\(C = f(x, y)\)</span>. This is a volumetricrepresentation for a 2D volume; it does not deal with any lines orcurves (which are surfaces in 2D). On the other hand, we can have asurface representation <span class="math inline">\(f(x,y) = 0\)</span>,a contour parametrized by an implicit function.</p><p>But how can we represent a complicated nonlinear function <span class="math inline">\(f(x, y)\)</span> on the computer? In 2025 we allknow the answer: deep neural networks. There is an experiment thatprobably every person really interested in deep learning has tried atleast once: approximate the function <span class="math inline">\(C=f(x,y)\)</span> with a fully-connected neural network (also known as amulti-layer perceptron or MLP) and train it on all pixels of an image.The dataset here consists of tuples <span class="math inline">\((x, y,C)\)</span> for all image pixels of a single image. Once trained, we usethis MLP to predict the color C for all pixels <span class="math inline">\((x, y)\)</span>, and thus we use <span class="math inline">\(f(x, y)\)</span> to render an image. <strong>Thispart is exactly what we will implement in Part 1.</strong> But let's seehow this naive approach works:</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_no_pe.png"></p><p>(From left to right illustrates the network output at 0, 100, 300,800, 1500 and 3000 training iterations)</p><p>It’s not particularly good, despite the neural network having moreparameters than pixels in the image, why? This representation has twoproblems:</p><ol type="1"><li><p>The raw coordinates <span class="math inline">\((x, y)\)</span>are not a good input representation for the neural network. The networkcannot easily learn high-frequency functions from such inputs. So thesolution is to use <strong>positional encoding</strong> (Fourierfeatures) to map the input coordinates <span class="math inline">\((x,y)\)</span> to a higher-dimensional space with more frequencycomponents. <strong>This is what we will implement in Part1.</strong></p></li><li><p>The ReLU-based MLPs can only represent piecewise linearfunctions. More sophisticated activation functions (e.g. Sigmoid, Sine)or network architectures (e.g. SIREN, Fourier Neural Operator) can helpto represent high-frequency functions better.</p></li></ol><p>This is the better result after applying positional encoding andsigmoid activation:</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png"></p><h3 id="functional-representation-in-3d-tsdf-and-nerf">FunctionalRepresentation in 3D: TSDF and NeRF</h3><p>How to represent 3D objects digitally? 3D representations follow thesame ideas as 2D ones.</p><p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/3d-object-representation.png"></p><p>Pixels in 3D become voxels. Point cloud in 3D is defined just like in2D. Polygonal meshes can be viewed as a special case of vector graphics.What about the functional representation? Once again, we have two typesof it: surface and volumetric.</p><p>Surface functional representation is about describing the surfaceswith the implicit equation <span class="math inline">\(f(x, y, z) =0\)</span>. This family of methods is called the (truncated) signeddistance function or (T)SDF. The volumetric representation is given bythe formula <span class="math inline">\(C=f(x, y, z)\)</span>, givingthe color <span class="math inline">\(C\)</span> of each 3D point <span class="math inline">\((x, y, z)\)</span>. We can think of these as“continuous voxels” or 3D translucent object made of colored jelly.</p><figure><img src="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-jelly.png" alt="NeRF as a translucent jelly"><figcaption aria-hidden="true">NeRF as a translucent jelly</figcaption></figure><p>This is basically what NeRF is, although in order to achieve betterresults, the actual NeRF adds two things: directional dependence anddensity.</p><h3 id="nerf-theory">NeRF Theory</h3><p>There are three main components of NeRF: scene representation,renderer and the training regime. NeRF represents a 3D scene as a 5Dfunction parametrized by a neural network:</p><p><img src="https://www.it-jim.com/wp-content/uploads/2023/05/nerf-scene-representation.png"></p><p>The inputs are the coordinates <span class="math inline">\(r=(x, y,z)\)</span> and the viewing direction <span class="math inline">\((\theta, \phi)\)</span>, often replaced by a unitdirection vector <span class="math inline">\(d=(d_1, d_2, d_3)\)</span>.The actual inputs to the MLP are the positional encodings of these twovectors. The output is the color <span class="math inline">\(C=(r, g,b)\)</span> and the density <span class="math inline">\(\sigma\)</span>.</p><p>But what about the lighting? The “standard” NeRF makes the followingstrict assumptions about the lighting:</p><ul><li>every point in the 3D scene emits light equally in all directions(i.e., no specular reflections);</li><li>every point in the 3D scene absorbs light according to its density<span class="math inline">\(\Sigma\)</span> only (i.e., no subsurfacescattering).</li></ul><p>As a result, the lighting conditions of the scene are frozen andcannot be changed after training.</p><h3 id="differentiable-volume-rendering">Differentiable VolumeRendering</h3><p>As we cannot perceive a 3D scene directly, what we typically want isto render it from a certain viewpoint or view, specified by the cameraparameters: intrinsic (focal length, image size) and extrinsic (cameraposition and direction). The result is a 2D image. Each camera pixelbecomes a ray in the 3D scene. The pixel color includes contributionsfrom all points along the ray given by the sum (or rather integral, asour model is continuous) over the points along the ray:</p><p><span class="math display">\[C(r) = \int_{t_n}^{t_f} T(t) \sigma(r(t)) c(r(t), d) dt,\]</span></p><p>where <span class="math inline">\(r(t) = o + t d\)</span> is thepoint along the ray at distance <span class="math inline">\(t\)</span>from the camera center <span class="math inline">\(o\)</span> indirection <span class="math inline">\(d\)</span>, <span class="math inline">\(c(r(t), d)\)</span> is the color at point <span class="math inline">\(r(t)\)</span> in direction <span class="math inline">\(d\)</span>, <span class="math inline">\(\sigma(r(t))\)</span> is the density at point<span class="math inline">\(r(t)\)</span>, and <span class="math inline">\(T(t) = \exp(-\int_{t_n}^{t} \sigma(r(s))ds)\)</span> is the accumulated transmittance from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span>. <span class="math inline">\(T(t)\)</span> gives the fraction of the lightintensity from the point t reaching the camera (the rest is absorbed).In the rendering slang it is also called “probability of the rayreaching the point t uninterrupted”.</p><p>In practice, NeRF uses a set of discrete points along the ray. Foreach point, get the <span class="math inline">\(c\)</span> and <span class="math inline">\(\sigma\)</span> from <span class="math inline">\(f(r(t), d)\)</span>, and then use sum toapproximate the integral. However this would lead to a fixed set of 3Dpoints, which could potentially lead to overfitting when we train theNeRF later on. On top of this, we want to introduce some smallperturbation to the points only during training, so that every locationalong the ray would be touched upon during training. this can beachieved by something like<code>t = t + (np.random.rand(t.shape) * t_width)</code> where<code>t</code> is set to be the start of each interval.</p><h2 id="deliverables">Deliverables</h2><ol start="0" type="1"><li><p>Camera Calibration and 3D Scanning</p><ul><li>3 screenshots of your camera frustums visualization in Viser:</li></ul><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_1.png"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_2.png"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180screenshot_3.png"></p></li><li><p>Fit a Neural Field to a 2D Image</p><ul><li><p>Model architecture report (number of layers, width, learningrate, and other important details)<code>"num_freqs": 10, "hidden_dim": 256, "num_layers": 3, "learning_rate": 0.01, "iterations": 3000, "batch_size": 10000,</code></p></li><li><p>Training progression visualization on both the provided testimage and one of your own images</p></li></ul><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress.png"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180progress_selfie.png"></p><ul><li><p>Final results for 2 choices of max positional encoding frequencyand 2 choices of width (2x2 grid) <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180grid.png"></p></li><li><p>PSNR curve for training on one image of your choice</p><ul><li>on the provided test image <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve.png"></li><li>on my own image <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180psnr_curve_selfie1.png"></li></ul></li></ul></li><li><p>Fit a Neural Radiance Field from Multi-view Images</p><ul><li><p>Implementation details (Pseudocodes)</p><ul><li>Create Rays from Cameras</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">transform</span>(<span class="hljs-params">c2w, x_c</span>):<br>    <span class="hljs-comment"># to transform camera coordinates to world coordinates</span><br>    x_w = c2w @ x_c<br>    <span class="hljs-keyword">return</span> x_w<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pixel_to_camera</span>(<span class="hljs-params">K, uv, s</span>):<br>    <span class="hljs-comment"># to transform pixel coordinates to camera coordinates</span><br>    <span class="hljs-comment"># K: intrinsic matrix, uv: pixel coordinates, s: depth</span><br>    fx, fy, cx, cy = K[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], K[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], K[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>], K[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>    u,v = uv[:,<span class="hljs-number">0</span>], uv[:,<span class="hljs-number">1</span>]<br>    x = (u - cx) * s / fx<br>    y = (v - cy) * s / fy<br>    z = s * np.ones_like(u)<br>    <span class="hljs-keyword">return</span> np.stack([x, y, z], axis=-<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pixel_to_ray</span>(<span class="hljs-params">K, c2w, uv</span>):<br>   <span class="hljs-comment"># Convert pixel coordinates to a ray (origin + normalized direction) in world space.</span><br>   point_camera = pixel_to_camera(K, uv, s=<span class="hljs-number">1.0</span>)  <span class="hljs-comment"># assume depth s=1.0</span><br>   point_world = transform(c2w, point_camera.T).T<br>   ray_origin = point_world<br>   ray_direction = point_world - c2w[:<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]<br>   ray_direction /= np.linalg.norm(ray_direction) <span class="hljs-comment"># normalize</span><br>   <span class="hljs-keyword">return</span> ray_origin, ray_direction<br></code></pre></td></tr></table></figure><ul><li>NeRF Network Structure</li></ul><p><img src="https://cal-cs180.github.io/fa25/hw/proj4/assets/mlp_nerf.png"></p></li><li><p>Rays Visualization <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray1.png"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_ray2.png"></p></li><li><p>Training progression visualization (rendered images at differenttraining iterations)</p><table><thead><tr><th>Iteration</th><th>Dataset</th><th>Rendered Image</th></tr></thead><tbody><tr><td>1000</td><td>Lego</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image1000.png"></td></tr><tr><td>3000</td><td>Lego</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image3000.png"></td></tr><tr><td>5000</td><td>Lego</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_image5000.png"></td></tr><tr><td>300</td><td>My Data</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola300.png"></td></tr><tr><td>1500</td><td>My Data</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola1500.png"></td></tr><tr><td>5000</td><td>My Data</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola5000.png"></td></tr></tbody></table></li><li><p>PSNR and Loss curves during training</p><ul><li><p>on the provided lego dataset <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180lego_log.png"></p></li><li><p>on my own dataset <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola_psnr.png"></p></li></ul></li><li><p>Final rendered GIFs</p><ul><li><p>on the provided lego dataset (batch = 8192, iterations = 5000)<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1808192_lego-ezgif.com-loop-count.gif"></p></li><li><p>on my own dataset (batch = 8192, sample =128, iterations = 5000)<img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180cola-ezgif.com-loop-count.gif"></p></li></ul></li></ul></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
      <tag>CS180</tag>
      
      <tag>CV</tag>
      
      <tag>NeRF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS180-Proj3A</title>
    <link href="/2025/10/08/CS180-Proj3A/index/"/>
    <url>/2025/10/08/CS180-Proj3A/index/</url>
    
    <content type="html"><![CDATA[<h1 id="cs180-lab-3---part-a">CS180 Lab 3 - Part A</h1><h2 id="overview">Overview</h2><p>It's trivial for us to do several types of geometric transformations:translation, rotation, scaling, skewing, etc. They can all berepresented as matrix multiplications. However, when we want to stitchtwo or more images from different viewpoints together, we need to use amore general transformation called <strong>homography</strong>.</p><h2 id="some-math">Some math</h2><h3 id="homography">Homography</h3><p>From Wikipedia:</p><blockquote><p>In the field of computer vision, any two images of the same planarsurface in space are related by a homography (assuming a pinhole cameramodel).</p></blockquote><figure><img src="https://upload.wikimedia.org/wikipedia/commons/7/79/Hauck_Neue_Constructionen_der_Perspective_fig1a.png" alt="Homography"><figcaption aria-hidden="true">Homography</figcaption></figure><p>The homography is a function <span class="math inline">\(h\)</span>that maps points into points, with the property that if points <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> are colinear, then the transformedpoints <span class="math inline">\(h(x)\)</span>, <span class="math inline">\(h(y)\)</span>, and <span class="math inline">\(h(z)\)</span> are <strong>also colinear</strong>.So the function <span class="math inline">\(h\)</span> can berepresented as a matrix multiplication:</p><p><span class="math display">\[\begin{bmatrix}x&#39;/w&#39; \\y&#39;/w&#39; \\1\end{bmatrix}\longleftrightarrow\begin{bmatrix}x&#39; \\y&#39; \\w&#39;\end{bmatrix}= H \begin{bmatrix}x \\y \\1\end{bmatrix}\]</span></p><p>where <span class="math inline">\(H\)</span> is a 3x3 matrix, and<span class="math inline">\((x&#39;, y&#39;, w&#39;)\)</span> are thetransformed coordinates in homogeneous coordinates.</p><h3 id="how-to-compute-h">How to compute H?</h3><p>First, we suppose H is:</p><p><span class="math display">\[H = \begin{bmatrix}h_{11} &amp; h_{12} &amp; h_{13} \\h_{21} &amp; h_{22} &amp; h_{23} \\h_{31} &amp; h_{32} &amp; h_{33}\end{bmatrix}\]</span></p><p>Then we assume we have <span class="math inline">\(n\)</span> pairsof corresponding points <span class="math inline">\((x_i, y_i)\)</span>and <span class="math inline">\((x&#39;_i, y&#39;_i)\)</span>, where<span class="math inline">\(i = 1, 2, ..., n\)</span>. Each pair givesus two equations:</p><p><span class="math display">\[x&#39;_i = \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32}y_i + h_{33}}\]</span></p><p><span class="math display">\[y&#39;_i = \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32}y_i + h_{33}}\]</span></p><p>so we can define <span class="math inline">\(h\)</span> as:</p><p><span class="math display">\[h = [h_{11}, h_{12}, h_{13}, h_{21},h_{22}, h_{23}, h_{31}, h_{32}, h_{33}]^T\]</span></p><p>Thus, we can get a system of linear equations <span class="math inline">\(Ah = 0\)</span>, where <span class="math inline">\(A\)</span> is a <span class="math inline">\(2n\times 9\)</span> matrix below:</p><p><span class="math display">\[A = \begin{bmatrix}x_1 &amp; y_1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -x&#39;_1 x_1 &amp;-x&#39;_1 y_1 &amp; -x&#39;_1 \\0 &amp; 0 &amp; 0 &amp; x_1 &amp; y_1 &amp; 1 &amp; -y&#39;_1 x_1 &amp;-y&#39;_1 y_1 &amp; -y&#39;_1 \\\cdots &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\x_n &amp; y_n &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -x&#39;_n x_n &amp;-x&#39;_n y_n &amp; -x&#39;_n \\0 &amp; 0 &amp; 0 &amp; x_n &amp; y_n &amp; 1 &amp; -y&#39;_n x_n &amp;-y&#39;_n y_n &amp; -y&#39;_n\end{bmatrix}\]</span></p><p>And we can solve this by SVD. The solution is the right singularvector corresponding to the smallest singular value of <span class="math inline">\(A\)</span>. (Finally, we need to reshape <span class="math inline">\(h\)</span> back to a 3x3 matrix <span class="math inline">\(H\)</span>.)</p><p>Also, from the above, we can see that we need at least 4 pairs ofcorresponding points to solve for <span class="math inline">\(H\)</span>. (every pair gives us 2 equations, andwe have 8 degrees of freedom)</p><hr><h2 id="a.1-shoot-and-digitize-pictures">A.1 Shoot and DigitizePictures</h2><p>I prepared three image pairs with large planar overlap:</p><ul><li>Bridge: the provided OpenCV panorama example for a controlledbaseline (panning around a riverside walkway).</li><li>Kitchen: two handheld shots of my kitchen counter taken a few inchesapart while rotating the camera about the optical center.</li><li>Table: a tabletop arrangement with books and a monitor, againacquired by pivoting the camera to keep the motion approximatelyprojective.</li></ul><p>Representative inputs are shown below (left/right are the two sourceimages in each pair):</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180s1.jpg"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180s2.jpg"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180k1.jpg"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180k2.jpg"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1801.jpg"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1802.jpg"></p><h2 id="a.2-recover-homographies">A.2 Recover Homographies</h2><p>Correspondences come either from manual clicking (JSON files in<code>test_data/.../*.json</code>). The correspondence are shownbelow:</p><ul><li><p>Bridge: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bridge_manual_corr.png"></p></li><li><p>Kitchen: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180manual_kitchen_corr.png"></p></li><li><p>Station: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180taskA2_correspondences_station.png"></p></li></ul><p>The estimated homographies are:</p><p><span class="math display">\[H_{bridge} = \begin{bmatrix}1.000517151994700926e+00 &amp; 1.934651821424284305e-05 &amp;-4.292858059440893612e+02 \\-1.347010553204711417e-04 &amp; 1.000425656962335097e+00 &amp;-1.110208515622688845e-02 \\-4.040777547417956760e-07 &amp; 9.404293585180129630e-07 &amp;1.000000000000000000e+00\end{bmatrix}\]</span></p><p><span class="math display">\[H_{kitchen} = \begin{bmatrix}1.383290749963294042e+00 &amp; 3.287087757504674745e-01 &amp;-4.954696332897179900e+02 \\-2.480568995413748590e-01 &amp; 1.054769919125850830e+00 &amp;4.261829110969327417e+02 \\5.365628407611597794e-04 &amp; -2.587039299089656324e-04 &amp;1.000000000000000000e+00\end{bmatrix}\]</span></p><p><span class="math display">\[H_{station} = \begin{bmatrix}2.051488129129804605e+00 &amp; -2.166269545193346968e-01 &amp;-7.714051347860786336e+02 \\5.373060026078243512e-01 &amp; 1.568850300184745361e+00 &amp;-3.035902657823175446e+02 \\1.117455412546467962e-03 &amp; -2.327306688189019295e-04 &amp;1.000000000000000000e+00\end{bmatrix}\]</span></p><p>The estimator (<code>utils.py::find_homography</code>)repeatedly:</p><ol type="1"><li>Samples four correspondences (with degeneracy checks).</li><li>Solves a normalized DLT system via SVD.</li><li>Scores inliers with a forward reprojection threshold.</li><li>Refits the homography on the winning consensus set.</li></ol><p>All three homographies preserve projective structure well enough forstitching.</p><h2 id="a.3-warp-and-rectify-images">A.3 Warp and Rectify Images</h2><h3 id="what-is-inverse-warping">What is Inverse Warping?</h3><p>Given a source image <code>im</code> and a homography <code>H</code>mapping source pixels to target pixels, the goal is to produce a newimage <code>out</code> that shows the source from the target'sperspective. The naive forward mapping approach (looping over everypixel in <code>im</code>, applying <code>H</code>, and writing to<code>out</code>) can leave holes because some target pixels may not behit. Instead, inverse warping loops over every pixel in<code>out</code>, applies the inverse homography <code>H^&#123;-1&#125;</code> tofind the corresponding pixel in <code>im</code>, and samples the colorthere. This guarantees that every pixel in <code>out</code> isfilled.</p><h3 id="nn-vs-bilinear">NN vs Bilinear</h3><p>The NN (nearest neighbour) works like this: <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*7uAm0FHBq_XmhcxS.jpg"></p><p>and bilinear interpolation works like this: <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*PJE50me94RGCfC2Z"></p><p>A simple view of NN vs bilinear warping of a chessboard pattern isshown below:</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180chess_output.png"></p><p>Obviously bilinear is smoother, but NN is faster and avoidsintroducing new colors (which can be helpful for segmentation). I usedbilinear for all final mosaics.</p><h3 id="practical-results">Practical Results</h3><ul><li><p>Bridge: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bridge_taskA3_comparison_bilinear.png"></p></li><li><p>Kitchen: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180kitchen_taskA3_comparison_bilinear.png"></p></li><li><p>Station: <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180station_taskA3_comparison_bilinear.png"></p></li></ul><h2 id="a.4-blend-images-into-a-mosaic">A.4 Blend Images into aMosaic</h2><p>The mosaicer (<code>blend_utils.build_mosaic</code>) projects everyimage into a common canvas, warps per-image alpha masks, and blendseither by:</p><ul><li>Feathering: simple weighted averaging driven by the warped alphafalloff.</li><li>Laplacian (two-level): blur the alphas, blend low/high bandsseparately to reduce ghosting.</li></ul><h3 id="procedure">Procedure</h3><ol type="1"><li><p>Unify the coordinate system and canvas size by projecting allimage corners and finding the bounding box. That is, multiply<code>H</code> with the four corners, find the final boundingbox.</p></li><li><p>Warp each image and its alpha mask into the common canvas usinginverse warping.</p></li><li><p>Use a linear falloff alpha mask that is 1 at the center and 0 atthe edges, then warp it to the canvas. Shows like this:</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180taskA4_alpha1_feather.png"></p></li><li><p>Blend the images using either feathering or Laplacian blending.For Laplacian blending, the low-frequency band is blended with theblurred alpha masks, and the high-frequency band is blended with thesharp alpha masks.</p></li></ol><h3 id="practical-results-1">Practical Results</h3><ul><li><p>Bridge (feathering): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_bridge.png"></p></li><li><p>Bridge (Laplacian): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_bridge.png"></p></li><li><p>Kitchen (feathering): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180k_taskA4_mosaic_feather.png"></p></li><li><p>Kitchen (Laplacian): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180k_taskA4_mosaic_laplacian2.png"></p></li><li><p>Station (feathering): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_match.png"></p></li><li><p>Station (Laplacian): <img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_match.png"></p></li></ul><p>It's obvious that current manual correspondences are not accurateenough, leading to noticeable artifacts in the blended results. Butdon't worry, in Part B we will auto detect and match features to getbetter correspondences.</p><h2 id="further-thoughts">Further Thoughts</h2><ol type="1"><li>I tried to implement spherical warping. The code refers to anopen-source implementation <a href="https://github.com/OpenStitching/stitching">here</a>. It worksbetter on Station image:</li></ol><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180spherical_result.jpg"></p><ol start="2" type="1"><li>Plan to implement exposure compensation and multi-band blending innext project.</li></ol><h1 id="cs180-lab-3---part-b">CS180 Lab 3 - Part B</h1><p>Now no more theory, let's just jump into tasks.</p><h2 id="task-1-harris-corner-detection">Task 1: Harris CornerDetection</h2><h4 id="harris-interest-point-detector---single-scale-implemented-with-the-given-sample-code-in-harris.py.">1.Harris Interest Point Detector - single scale: implemented with thegiven sample code in <code>harris.py</code>.</h4><p>Results:</p><table><thead><tr><th>Input Image</th><th>Detected Corners</th></tr></thead><tbody><tr><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180Knight250.png"></td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out.png"></td></tr><tr><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180blue_man.png"></td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180blue_man_out.png"></td></tr></tbody></table><h4 id="adaptive-non-maximal-suppression-anms">2. Adaptive Non-MaximalSuppression (ANMS)</h4><p>The key idea of ANMS in the paper is to select corners that are bothstrong and well-distributed. The algorithm works as follows:</p><ol type="1"><li>Compute the Harris corner response for all pixels in the image. Foreach corner point, calculate its minimum suppression radius <span class="math inline">\(r_i\)</span>: <span class="math display">\[r_i =\min_{j} \{ d_{ij} | h_j &gt; c_{robust} * h_i \}\]</span> where <span class="math inline">\(d_{ij}\)</span> is the Euclidean distance betweencorner points <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, <span class="math inline">\(h_i\)</span> and <span class="math inline">\(h_j\)</span> are their Harris corner responses,and <span class="math inline">\(c_{robust}\)</span> is a robustnessratio (typically set to 0.9).</li><li>Sort the corner points by their suppression radii in descendingorder. The bigger the radius, the more isolated and stronger the corneris.</li><li>Select the top N corner points with the largest suppressionradii.</li></ol><p>Results:</p><table><thead><tr><th>Parameters</th><th>Detected Corners (ANMS)</th></tr></thead><tbody><tr><td>N=500, c_robust=0.9</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out_500.png"></td></tr><tr><td>N=300, c_robust=0.9</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out_300.png"></td></tr><tr><td>N=100, c_robust=0.9</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out_100_.png"></td></tr><tr><td>N=500, c_robust=0.7</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out_r07.png"></td></tr><tr><td>N=500, c_robust=1.1</td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180knight_out_r11.png"></td></tr></tbody></table><h2 id="task-2-feature-descriptor">Task 2: Feature Descriptor</h2><p>The only thing worth mentioning is to bias/gain-normalize thedescriptor patch. The equations are as follows:</p><p><span class="math display">\[I_{norm} = \frac{I - \mu}{\sigma +\epsilon}\]</span></p><p>where <span class="math inline">\(\mu\)</span> is the mean intensityof the patch, <span class="math inline">\(\sigma\)</span> is thestandard deviation, and <span class="math inline">\(\epsilon\)</span> isa small constant (e.g., 1e-10) to prevent division by zero. Thisnormalization ensures that the descriptor is invariant to changes inbrightness and contrast, making it more robust for matching featuresacross different images.</p><p>So let's just see the results.</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180Knight250_descriptors.png"></p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180Knight250_patches.png"></p><h2 id="task-3-feature-matching">Task 3: Feature Matching</h2><p>The main idea of the Lowe's ratio test likes this:</p><ol type="1"><li>For each descriptor in the first image <span class="math inline">\(f_i\)</span>, compute the Euclidean distance toall descriptors in the second image <span class="math inline">\(g_j\)</span>.</li><li>Find the two smallest distances <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>.</li><li>If <span class="math inline">\(d_1 / d_2 &lt;\text{ratio\_thresh}\)</span>, then we consider the match valid.</li></ol><p><strong>Why this?</strong></p><p>I think the metaphor given by our professor is quite good:</p><blockquote><p>If you are tangling with two boyfriend candidates and cannot decidewhich one is better, that may indicate that neither of them is reallysuitable for you. However, if one candidate is clearly better than theother, then you can be more confident in your choice.</p></blockquote><p>Results:</p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180s1.jpg_s2.jpg_matches.png"></p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS1801.jpg_2.jpg_matches.png"></p><h2 id="task-4-image-stitching-with-ransac">Task 4: Image Stitching withRANSAC</h2><p>This part is already done in Part A. So let's just see the finalresults.</p><table><thead><tr><th>manual matches</th><th>auto matches with feature matching</th></tr></thead><tbody><tr><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_match.png"></td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180result.jpg"></td></tr><tr><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180bad_bridge.png"></td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180taskA4_mosaic_laplacian2.png"></td></tr><tr><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180new_kitchen.png"></td><td><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/CS180k_taskA4_mosaic_feather.png"></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
      <tag>CS180</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>组会报告汇总</title>
    <link href="/2025/05/27/group_meeting/group_meeting/"/>
    <url>/2025/05/27/group_meeting/group_meeting/</url>
    
    <content type="html"><![CDATA[<h3 id="写在前面">写在前面</h3><p>苦逼组会报告记录，等哪天闲了把 Notion 的链接和图片全部导出成 Markdown搬过来</p><h2 id="section">2025.03.01</h2><h3 id="上周工作汇报">上周工作汇报</h3><ol type="1"><li>熟悉手语项目开发流程</li><li>CTC 算法，Non-Peaky CTC 算法</li><li>VAP（Visual Alignment Pretrain）论文研究</li></ol><p><a href="https://www.notion.so/Visual-Alignment-Pre-training-for-Sign-Language-Translation-1a55661a96728019a0f7cb5db467699f?pvs=21">VisualAlignment Pre-training for Sign Language Translation</a></p><h3 id="下周工作内容">下周工作内容</h3><ol type="1"><li>复现和测试 VAP 的方法</li><li>熟悉大模型的训练步骤</li></ol><h2 id="section-1">2025.03.08</h2><h3 id="这周的工作">这周的工作</h3><ul><li>ctc 工作原理和尖峰的解决方案之一</li></ul><p><a href="https://www.notion.so/ctc-1af5661a9672800ba39cd75cce756b39?pvs=21">ctc</a></p><ul><li>代码实际测试（正在）</li></ul><h3 id="下周准备">下周准备</h3><ul><li>看下改过 loss 的训练效果</li></ul><h2 id="section-2">2025.03.14</h2><h3 id="上一周的工作">上一周的工作</h3><ul><li>验证了简单的 label priors（多除一个先验概率）对 CTC训练的作用很小（至少对 loss 来说，也可能是调的参数不对）</li></ul><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:1c0020c6-6696-4f3d-a3e2-1c83a6d0e5ec:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>实现了 VAP loss 的代码，但现有的 model pipeline需要一定程度的修改（整句话词 embedding，删去 QFormer，etc）</li></ul><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:ac93bbe9-8253-4704-a808-f07f0e3dd62b:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>问题：</p><ul><li>用到的 mbart embedding 的分词和我们的分词肯定不太一样</li><li>如果直接对我们的 gloss 词本embedding，那可能有的词会被分开，长度不一样</li></ul><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:f0708809-1d9d-43f5-a626-6ae1fccca918:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:92c452e2-bf0a-4377-88e8-db4007cebe7a:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>尝试使用<strong>Sentence-BERT</strong></li></ul><h3 id="下一周工作">下一周工作</h3><ul><li>也许可以再调调 label priors 方法的参数</li><li>用 sentence-bert 把现在的单词本 encoding 一遍，然后开始使用 VAP的方法训练</li></ul><h2 id="section-3">2025.03.21</h2><h3 id="上一周的工作-1">上一周的工作</h3><ul><li>VAP 训练</li></ul><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:ee7a4824-02f2-461b-8c6b-aaf1db863e87:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><p>但实际切片效果不行</p><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:2e063b2e-18c5-4dca-b6e3-3538ecb21ce1:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><figure><img src="/2025/05/27/group_meeting/group_meeting//attachment:ab538ce2-ce45-4e6c-8f40-b66422076a26:image.png" alt="image.png"><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>CTC</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
      <tag>组会</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数笔记</title>
    <link href="/2025/01/14/linear-algebra/linear-algebra/"/>
    <url>/2025/01/14/linear-algebra/linear-algebra/</url>
    
    <content type="html"><![CDATA[<h2 id="preface">Preface</h2><p>The note mainly referenced to <a href="https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/">MIT18.06 Linear Algebra</a>, <a href="https://space.bilibili.com/88461692/lists/1528927?type=series">3blue1brown'sEssence of Linear Algebra</a>, and of course, <a href="https://www.skodleracks.co.uk/">the course of Linear Algebra I inShanghaiTech University</a>.</p><h2 id="unit-1-vectors-and-matrices">Unit 1: Vectors and Matrices</h2><h3 id="the-geometry-of-linear-equations">1.1 The Geometry of LinearEquations</h3><h4 id="what-is-a-matrix">What is a matrix?</h4><ul><li><ol type="I"><li>a coefficient view(线性方程组的系数):</li></ol><p><span class="math display">\[\begin{cases}2x - y = 0\\-x + 2y = 3\end{cases}\quad\Rightarrow  \quad  \begin{bmatrix}  2 &amp; -1\\  -1 &amp; 2  \end{bmatrix}  \begin{bmatrix}  x\\  y  \end{bmatrix}  =  \begin{bmatrix}  0\\  3  \end{bmatrix}\]</span></p></li><li><ol start="2" type="I"><li>a row view(多条线性方程的函数图像): <span class="math inline">\(\begin{cases}2x - y = 0\\-x + 2y = 3\end{cases}\)</span></li></ol><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/p1_1_row_view.svg"></p></li><li><ol start="3" type="I"><li>a column view:</li></ol><p>the linear combination of vectors(许多向量的线性组合)</p><p><span class="math display">\[\begin{bmatrix}2\\-1\end{bmatrix}x+\begin{bmatrix}-1\\2\end{bmatrix}y=\begin{bmatrix}0\\3\end{bmatrix}\]</span></p></li><li><ol start="4" type="I"><li><em>the basis transformation of the space</em>(这一部分将在 Unit3中的线性变换中给出更详细的解释，但我们应该在一开始就有这样的 intuition)<p align="center">将基向量变化为给定矩阵的列向量<video src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/BasisTransform.mp4" autoplay="true" controls="controls" width="100%"></video></p></li></ol></li></ul><h3 id="elimination">1.2 Elimination</h3><h4 id="gaussian-jordan-elimination高斯-约旦消元法">Gaussian-JordanElimination(高斯-约旦消元法)</h4><p>所谓消元法，其实可以看作是初中生都会的求解多元一次方程组:</p><p>e.g.</p><p><span class="math display">\[\left\{\begin{array}{rcl}  x &amp;+ ~2y + z &amp;= 2 \\  3x &amp;+ ~8y + z &amp;= 12 \\     &amp; \quad 4y + z &amp;= 2\end{array}\right.\Rightarrow\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 1 &amp; 2 \\3 &amp; 8 &amp; 1 &amp; 12 \\0 &amp; 4 &amp; 1 &amp; 2\end{array}\right]\]</span></p><p>这里，我们把方程组等号左边的系数和右边的常数写在一个矩阵中，这个矩阵称为增广矩阵(AugmentedMatrix)。</p><p>我们应该如何求解这个方程组呢？只看左边的方程组，最直接的思路就是消元法，用第二个方程减去第一个方程，这样就消去了<span class="math inline">\(z\)</span>，再通过第三个方程得到<span class="math inline">\(y\)</span>和<span class="math inline">\(z\)</span>的关系式，最后解出所有的未知数。</p><p>高斯消元法的本质就是这样，只不过我们最好按照更加固定的步骤来进行，保证对一切方程组都能按部就班地求解(有了按部就班的方法，剩下的计算就是计算机的活了)。</p><p>具体的步骤如下:</p><ul><li><p>Step 1:从当前行(第一行)开始，找到第一个非零元素，称为主元素(Pivot)，如果主元素为0，则交换行，使得主元素不为 0。</p><p><span class="math display">\[\left[\begin{array}{ccc|c}\overset{\text{Pivot}}{\fcolorbox{red}{white}{1}} &amp; 2 &amp; 1 &amp;2 \\3 &amp; 8 &amp; 1 &amp; 12 \\0 &amp; 4 &amp; 1 &amp; 2\end{array}\right]\]</span></p></li><li><p>Step 2: 用主元素消去下面的元素，使得下面每一行、同一列(Pivotcolumn)的元素为 0，其他列的元素则加上/减去相应倍数的第一行元素。</p><p><span class="math display">\[\underset{r_3&#39;=r_3}{\overset{r_2&#39;=r_2-3r_1}{\longrightarrow}}\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 1 &amp; 2 \\0 &amp; 2 &amp; -2 &amp; 6 \\0 &amp; 4 &amp; 1 &amp; 2\end{array}\right]\]</span></p></li><li><p>Step 3: 换下一行，重复 Step 1 和 Step 2，直到所有的主元素都为1，且主元素下面的元素都为 0。</p><p><span class="math display">\[\overset{r_3&#39;=r_3-2r_2}{\longrightarrow}\left[\begin{array}{ccc|c}{1} &amp; 2 &amp; 1 &amp; 2 \\0 &amp; \overset{\text{new Pivot}}{\fcolorbox{red}{white}{2}} &amp; -2&amp; 6 \\0 &amp; 0 &amp; 5 &amp; -10\end{array}\right]\]</span></p><p>此时，我们就得到了一个<strong>上三角矩阵</strong>(Upper TriangularMatrix)，这个矩阵的特点是主对角线以下的元素都为 0 :</p><p><span class="math display">\[\begin{bmatrix}1 &amp; 2 &amp; 1\\\color{red}{0} &amp; 2 &amp; -2\\\color{red}{0} &amp; \color{red}{0} &amp; 5\end{bmatrix}\]</span></p><p>如果我们把这个矩阵的右边的常数项也写在矩阵中，那么这个矩阵就是一个<strong>行阶梯形矩阵</strong>(RowEchelon Form)：</p><!-- $$\left[\begin{array}{ccc|c}1 & 2 & 1 & 2 \\0 & 2 & -2 & 6 \\0 & 0 & 5 & -10\end{array}\right]$$ --><p align="center"></p><p><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/20250116023025111.png"></p><p></p><p>实际上，到这一步时，我们算是完成了 Gauss-Jordan消元法的第一部分，也就是"GaussionElimination"，即将矩阵变换为上三角矩阵。接下来，我们需要将这个矩阵变换为<strong>行最简形矩阵</strong>(RowReduced Echelon Form)。</p></li><li><p>Step 4: 回忆最基本的方程组求解办法，现在我们相当于有方程组：</p><p><span class="math display">\[\begin{cases}x + 2y + z = 2\\2y - 2z = 6\\5z = -10\end{cases}\]</span></p><p>我们应该从最后一行开始，逐步解出所有的未知数，这个过程称为<strong>回代(Substitution)</strong>。为了方便回代过程，我们可以先把每行归一化：</p><p><span class="math display">\[\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 1 &amp; 2 \\0 &amp; 1 &amp; -1 &amp; 3 \\0 &amp; 0 &amp; 1 &amp; -2\end{array}\right]\]</span></p><p>然后，我们可以模仿一开始的消元过程，从最后一行开始，逐步回代：</p><p><span class="math display">\[\underset{r_1&#39;=r1-r3}{\overset{r_2&#39; = r_2+r_3}{\longrightarrow}}\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 0 &amp; 4 \\0 &amp; 1 &amp; 0 &amp; 1 \\0 &amp; 0 &amp; 1 &amp; -2\end{array}\right]\overset{r_1&#39;=r_1-2r_2}{\longrightarrow}\left[\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; 2 \\0 &amp; 1 &amp; 0 &amp; 1 \\0 &amp; 0 &amp; 1 &amp; -2\end{array}\right]\]</span></p><p>把这个矩阵变换为<strong>行最简形矩阵</strong>(Row Reduced EchelonForm)：</p><p><span class="math display">\[\left[\begin{array}{ccc|c}\overset{\text{leading} ~1}{\fcolorbox{red}{white}{1}} &amp; 0 &amp; 0&amp; 2 \\0 &amp; \overset{\text{leading} ~1}{\fcolorbox{red}{white}{1}} &amp; 0&amp; 1 \\0 &amp; 0 &amp; \overset{\text{leading} ~1}{\fcolorbox{red}{white}{1}}&amp; -2\end{array}\right]:\begin{cases}  \text{The leading entry in each nonzero row is 1 (called a leadingone).} \\  \quad \text{每个非零行的前导元素为 1。} \\  \text{Each leading 1 is the only nonzero entry in its column.} \\  \quad \text{每个前导 1 是其列中唯一的非零元素。}\end{cases}\]</span></p><p>注意，这个 reduce echelon form 是唯一的，就是我们的方程组的解： $</p><span class="math display">\[\begin{cases}x = 2\\y = 1\\z = -2\end{cases}\]</span><p>$</p></li></ul><h4 id="more-about-row-ehcelon-form-reduced-row-echelon-form">More aboutRow Ehcelon Form &amp; Reduced Row Echelon Form</h4><ul><li>row echelon form 并不一定是 triangular的，其可以被视为一种弱化或者是推广的 triangular matrix，例如：</li></ul><p align="center"><img src="https://raw.githubusercontent.com/Lceoliu/blog-img/main/posts/linear_algebra20250116033035751.png"><br> 不是 triangular 的 row echelon form</p><ul><li><p>一个矩阵消元得到的 row echelon form 并不一定是唯一的，但是reduced row echelon form 是唯一的。</p></li><li><p>根据 reduced row echelon form 的性质，我们可以把任意的 reducedrow echelon form 矩阵写成如下形式：</p><p><span class="math display">\[\begin{bmatrix}I &amp; X\\0 &amp; 0\end{bmatrix}\]</span></p><p>其中，<span class="math inline">\(I\)</span>是<strong>单位矩阵</strong>（有且只有主对角线上元素为 1，其余元素为0），<span class="math inline">\(X\)</span>是自由变量的系数矩阵。</p></li></ul><h3 id="matrix-multiplication-and-inverse">1.3 Matrix Multiplication andInverse</h3><h4 id="matrix-multiplication">Matrix Multiplication</h4><ul><li><ol type="I"><li>为什么我们需要矩阵乘法？</li></ol><p>在 1.1 中，我们就提到了 coefficient view 和 column view，例如<span class="math inline">\(\begin{bmatrix}2 &amp; -1\\-1 &amp;2\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}0\\3\end{bmatrix}\)</span>，这个式子就是一个矩阵乘法；而我们在column view 中，也把它理解为两个向量的线性组合，即<span class="math inline">\(\begin{bmatrix}2\\-1\end{bmatrix}x+\begin{bmatrix}-1\\2\end{bmatrix}y=\begin{bmatrix}0\\3\end{bmatrix}\)</span>，实际上，这两个视角是等价的：矩阵的乘法，都可以视为不同向量的不同线性组合。</p></li><li><ol start="2" type="I"><li>矩阵乘法的定义：</li></ol><p>对于矩阵<span class="math inline">\(A\in\mathbb{R}^{m\times\boxed{n}}\)</span>和<span class="math inline">\(B\in\mathbb{R}^{\boxed{n} \timesp}\)</span>，其乘积<span class="math inline">\(C=AB\in\mathbb{R}^{m\timesp}\)</span>的定义为：</p><p><span class="math display">\[C_{ij}=\sum_{k=1}^nA_{ik}B_{kj}\quad \text{或者说} \quadC_{ij}=\overrightarrow{\text{row}_i(A)}\cdot\overrightarrow{\text{col}_j(B)}\]</span></p><p>（注：所谓<span class="math inline">\(A\in\mathbb{R}^{m\timesn}\)</span>，表示矩阵<span class="math inline">\(A\)</span>的行数为<span class="math inline">\(m\)</span>，列数为<span class="math inline">\(n\)</span>，所有元素都属于实数域<span class="math inline">\(\mathbb{R}\)</span>；<span class="math inline">\(\overrightarrow{\text{row}_i(A)}\)</span>代表将矩阵<span class="math inline">\(A\)</span>的第<span class="math inline">\(i\)</span>行视为一个向量）</p><p>很显然，我们发现，<strong>矩阵乘法是不满足交换律的</strong>，即<span class="math inline">\(AB\neq BA\)</span>。</p></li><li><ol start="3" type="I"><li>列视角理解：</li></ol><p>对于矩阵乘法<span class="math inline">\(AB\)</span>，我们可以把<span class="math inline">\(A\)</span>视为一组列向量的集合，那么<span class="math inline">\(AB\)</span>中的每一列都是<span class="math inline">\(A\)</span>中所有列向量的线性组合，且系数由<span class="math inline">\(B\)</span>中的对应列决定。</p><p>例如，对于矩阵<span class="math inline">\(A=\begin{bmatrix}1 &amp;2\\3 &amp; 4\end{bmatrix}\)</span>和<span class="math inline">\(B=\begin{bmatrix}5 &amp; 6\\7 &amp;8\end{bmatrix}\)</span>，我们可以把<span class="math inline">\(A\)</span>视为两个列向量<span class="math inline">\(\begin{bmatrix}1\\3\end{bmatrix}\)</span>和<span class="math inline">\(\begin{bmatrix}2\\4\end{bmatrix}\)</span>，那么<span class="math inline">\(AB\)</span>中的第一列就是<span class="math inline">\(A\)</span>中两个列向量的线性组合，而系数则来自<span class="math inline">\(B\)</span>中对应的第一列元素<span class="math inline">\(5\)</span>和<span class="math inline">\(7\)</span>，即<span class="math inline">\(5\begin{bmatrix}1\\3\end{bmatrix}+7\begin{bmatrix}2\\4\end{bmatrix}=\begin{bmatrix}19\\43\end{bmatrix}\)</span>。同理，<span class="math inline">\(AB\)</span>中的第二列就是两个列向量以<span class="math inline">\(B\)</span>中对应的<span class="math inline">\(6\)</span>和<span class="math inline">\(8\)</span>为系数的线性组合，即<span class="math inline">\(6\begin{bmatrix}1\\3\end{bmatrix}+8\begin{bmatrix}2\\4\end{bmatrix}=\begin{bmatrix}22\\50\end{bmatrix}\)</span>。即：</p><p><span class="math display">\[AB=\begin{bmatrix}1 &amp; 2\\3 &amp; 4\end{bmatrix}\begin{bmatrix}5&amp; 6\\7 &amp; 8\end{bmatrix}=\begin{bmatrix}19 &amp; 22\\43 &amp;50\end{bmatrix}\]</span></p></li><li><ol start="4" type="I"><li>行视角理解：</li></ol><p>对于矩阵乘法<span class="math inline">\(AB\)</span>，我们可以把<span class="math inline">\(B\)</span>视为一组行向量的集合，那么<span class="math inline">\(AB\)</span>中的每一行都是<span class="math inline">\(B\)</span>中所有行向量的线性组合，且系数由<span class="math inline">\(A\)</span>中的对应行决定。</p><p>例如，对于矩阵<span class="math inline">\(A=\begin{bmatrix}1 &amp;2\\3 &amp; 4\end{bmatrix}\)</span>和<span class="math inline">\(B=\begin{bmatrix}5 &amp; 6\\7 &amp;8\end{bmatrix}\)</span>，我们可以把<span class="math inline">\(B\)</span>视为两个行向量<span class="math inline">\(\begin{bmatrix}5 &amp;6\end{bmatrix}\)</span>和<span class="math inline">\(\begin{bmatrix}7&amp; 8\end{bmatrix}\)</span>，那么<span class="math inline">\(AB\)</span>中的第一行就是<span class="math inline">\(B\)</span>中两个行向量的线性组合，而系数则来自<span class="math inline">\(A\)</span>中对应的第一行元素<span class="math inline">\(1\)</span>和<span class="math inline">\(2\)</span>，即<span class="math inline">\(1\begin{bmatrix}5 &amp;6\end{bmatrix}+2\begin{bmatrix}7 &amp; 8\end{bmatrix}=\begin{bmatrix}19&amp; 22\end{bmatrix}\)</span>。同理，<span class="math inline">\(AB\)</span>中的第二行就是两个行向量以<span class="math inline">\(A\)</span>中对应的<span class="math inline">\(3\)</span>和<span class="math inline">\(4\)</span>为系数的线性组合，即<span class="math inline">\(3\begin{bmatrix}5 &amp;6\end{bmatrix}+4\begin{bmatrix}7 &amp; 8\end{bmatrix}=\begin{bmatrix}43&amp; 50\end{bmatrix}\)</span>。即：</p><p><span class="math display">\[AB=\begin{bmatrix}1 &amp; 2\\3 &amp; 4\end{bmatrix}\begin{bmatrix}5&amp; 6\\7 &amp; 8\end{bmatrix}=\begin{bmatrix}19 &amp; 22\\43 &amp;50\end{bmatrix}\]</span></p></li><li><ol start="22" type="A"><li>块(block)视角理解：</li></ol><p>如果我们把一个大的矩阵分为几个小的块，那么矩阵乘法就可以看作是块的乘法：</p><p><span class="math display">\[\begin{bmatrix}A_{11} &amp; A_{12}\\A_{21} &amp; A_{22}\end{bmatrix}\begin{bmatrix}B_{11} &amp; B_{12}\\B_{21} &amp; B_{22}\end{bmatrix}=\begin{bmatrix}A_{11}B_{11}+A_{12}B_{21} &amp; A_{11}B_{12}+A_{12}B_{22}\\A_{21}B_{11}+A_{22}B_{21} &amp; A_{21}B_{12}+A_{22}B_{22}\end{bmatrix}\]</span></p><p>一种可能的分块：</p><p><span class="math display">\[\begin{bmatrix}1 &amp; 2 &amp; 3 &amp; 4\\5 &amp; 6 &amp; 7 &amp; 8\end{bmatrix}\longrightarrow\left[\begin{array}{cc|cc}1 &amp; 2 &amp; 3 &amp; 4\\\hline5 &amp; 6 &amp; 7 &amp; 8\end{array}\right]\longrightarrow\begin{bmatrix}A_{11} &amp; A_{12}\\A_{21} &amp; A_{22}\end{bmatrix}\]</span></p></li><li><ol start="6" type="I"><li>高斯消元中的矩阵乘法</li></ol><p>在(III)和(IV)中，我们提到了矩阵乘法的列视角和行视角，为什么我们要这样理解矩阵乘法呢？这是因为在高斯消元法中，我们用一行加上或减去另一行的倍数，实际上就是在做矩阵乘法的行视角理解：</p><p><span class="math display">\[\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 1 &amp; 2 \\3 &amp; 8 &amp; 1 &amp; 12 \\0 &amp; 4 &amp; 1 &amp; 2\end{array}\right]\overset{r_2&#39;=r_2-3r_1}{\longrightarrow}\left[\begin{array}{ccc|c}1 &amp; 2 &amp; 1 &amp; 2 \\0 &amp; 2 &amp; -2 &amp; 6 \\0 &amp; 4 &amp; 1 &amp; 2\end{array}\right]\]</span></p><p>等价于：</p><p><span class="math display">\[\underset{E_1}{\begin{bmatrix}1 &amp; 0 &amp; 0\\-3 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1\end{bmatrix}}\underset{A}{\begin{bmatrix}1 &amp; 2 &amp; 1 &amp; 2 \\3 &amp; 8 &amp; 1 &amp; 12 \\0 &amp; 4 &amp; 1 &amp; 2\end{bmatrix}}=\underset{B}{\begin{bmatrix}1 &amp; 2 &amp; 1 &amp; 2 \\0 &amp; 2 &amp; -2 &amp; 6 \\0 &amp; 4 &amp; 1 &amp; 2\end{bmatrix}}\]</span></p><p>自然地，既然我们可以把“第二行减去三倍的第一行”理解为一个矩阵乘法，那么我们也可以把<a href="#gaussian-jordan-elimination高斯-约旦消元法">高斯消元法的所有步骤</a>理解为许多矩阵乘法的组合：</p><p><span class="math display">\[\underset{\underset{第二步}{E_2}}{\begin{bmatrix}1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0\\0 &amp; -2 &amp; 1\end{bmatrix}}\underset{\underset{第一步}{E_1}}{\begin{bmatrix}1 &amp; 0 &amp; 0\\-3 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1\end{bmatrix}}\underset{A}{\begin{bmatrix}1 &amp; 2 &amp; 1 &amp; 2 \\3 &amp; 8 &amp; 1 &amp; 12 \\0 &amp; 4 &amp; 1 &amp; 2\end{bmatrix}}=\underset{U}{\begin{bmatrix}1 &amp; 2 &amp; 1 &amp; 2 \\0 &amp; 2 &amp; -2 &amp; 6 \\0 &amp; 0 &amp; 5 &amp; -10\end{bmatrix}}\]</span></p><p>这里，<span class="math inline">\(E_1\)</span>和<span class="math inline">\(E_2\)</span>分别是高斯消元法的第一步和第二步的矩阵，<span class="math inline">\(A\)</span>是原始的增广矩阵，<span class="math inline">\(U\)</span>是上三角矩阵。当然，我们也可以把<span class="math inline">\(E_1\)</span>和<span class="math inline">\(E_2\)</span>合并为一个矩阵<span class="math inline">\(E=E_2E_1\)</span>，那么<span class="math inline">\(U=EA\)</span>。</p><p>另外，我们很容易能注意到，所有的<span class="math inline">\(E_i\)</span>都是<strong>下三角矩阵(LowerTriangularMatrix)</strong>，这很直观，因为我们在消元的过程中，只是用下面的行减去上面的行的倍数，而不会用上面的行减去下面的行的倍数，自然地，<span class="math inline">\(E\)</span>也是下三角矩阵。</p></li></ul><h4 id="matrix-inverse">Matrix Inverse</h4><p>​既然我们有了乘法，那么肯定也得有除法吧！（之后我们会从一个更抽象的层面去理解为什么有了乘就得有除）</p><p>​ 如何定义矩阵的除法？回忆实数的乘除，<span class="math inline">\(\frac{a}{b} = ab^{-1} = a \times \mathbf{1} \divb\)</span>，这看上去好像是废话是不是，但这正是我们对除法最根本的定义：乘以这个数的倒数（Inverse）。而倒数的定义就是，这个数乘以他的倒数等于单位<span class="math inline">\(\mathbf{1}\)</span>。</p><p>​那么，在矩阵中，单位一是什么呢？不用说你也应该猜到了，就是我们之前所说的单位矩阵<span class="math inline">\(\mathbf{I}\)</span>，不过和实数中的1不同的是，<span class="math inline">\(\mathbf{I}\)</span>并不是一个完全确定的数字，而可以是任意大小(只要符合<span class="math inline">\(R^{n \times n}\)</span>以及对角线上元素全为<span class="math inline">\(1\)</span>而其余元素为<span class="math inline">\(0\)</span>即可)。因此，当我们对任意一个矩阵<span class="math inline">\(A \in R^{m \times n}\)</span>说<span class="math inline">\(A^{-1}\)</span>时，其满足： <span class="math display">\[A^{-1}A=\mathbf{I},\quad AA^{-1}=\mathbf{I}\]</span> (注意：<span class="math inline">\(A^{-1}A \in R^{n \timesn}\)</span>而<span class="math inline">\(AA^{-1} \in R^{m \timesm}\)</span>，但出于习惯和方便，我们都简写为<span class="math inline">\(\mathbf{I}\)</span>)</p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>独立游戏品鉴与分析</title>
    <link href="/2024/09/30/page-1/page-1/"/>
    <url>/2024/09/30/page-1/page-1/</url>
    
    <content type="html"><![CDATA[<h2 id="写在最前面">写在最前面：</h2><p>记录一下玩过的许多优秀独立游戏，并分析一下有趣的点，希望自己能长期更新。</p><p>每个游戏都会给出一个我自创的打分规则，规则如下：</p><ol type="1"><li>玩法<ul><li>创新性</li><li>重玩性</li><li>自由度</li><li>操作感</li><li>沉浸感</li><li>细节设计</li></ul></li><li>画面<ul><li>风格化</li><li>统一程度</li><li>画面质量</li></ul></li><li>音乐<ul><li>适配度</li><li>全局影响</li><li>情绪影响</li></ul></li><li>剧情<ul><li>创新度</li><li>人物刻画</li><li>背景完善程度</li><li>情绪影响</li><li></li></ul></li></ol><p>P.S.以下所有品鉴纯属个人口味，也许某天再回来看这时候写的分析也会觉得很愚蠢</p><h3 id="luck-be-a-landlord-幸运房东">Luck be a Landlord (幸运房东)</h3><h3 id="shapez.io">Shapez.io</h3><h3 id="vampire-survivors">Vampire Survivors</h3><h3 id="section"></h3>]]></content>
    
    
    
    <tags>
      
      <tag>GameDev</tag>
      
      <tag>随笔</tag>
      
      <tag>独立游戏</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Games101自学笔记</title>
    <link href="/2024/09/14/page/page/"/>
    <url>/2024/09/14/page/page/</url>
    
    <content type="html"><![CDATA[<h2 id="games101-自学笔记">Games101 自学笔记</h2><h3 id="l12">L1,2</h3><p>略</p><hr><h3 id="l34-transformation">L3,4 Transformation</h3><h4 id="基础变换的矩阵形式">基础变换的矩阵形式</h4><p>为了使平移变换可以写成线性变换，所以我们要使用 4 维矩阵表示 3维空间的向量、点和变换；</p><p>我们把向量记为<span class="math display">\[\begin{bmatrix}x\\  y\\  z\\  0\end{bmatrix}\]</span>，把点记为<span class="math display">\[\begin{bmatrix}x\\  y\\  z\\  1\end{bmatrix}\]</span>，是因为：</p><ul><li>向量平移后应与原向量无区别</li><li>方便点的坐标统一（认为<span class="math display">\[\begin{bmatrix}x\\  y\\  z\\  1\end{bmatrix}\]</span>与<span class="math display">\[\begin{bmatrix}2x\\  2y\\  2z\\  2\end{bmatrix}\]</span>无区别）</li></ul><p>所以，我们可以得出所有基础变换的变换矩阵：</p><ol type="1"><li><p>平移矩阵</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 &amp; dx\\0 &amp; 1 &amp; 0 &amp; dy\\0 &amp; 0 &amp; 1 &amp; dz\\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p></li><li><p>缩放矩阵</p></li></ol><p>​</p><p><span class="math display">\[\left[\begin{array}{c}tx &amp; 0 &amp; 0 &amp; 0\\0 &amp; ty &amp; 0 &amp; 0\\0 &amp; 0 &amp; tz &amp; 0\\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p><ol start="3" type="1"><li><p>旋转矩阵（右手坐标系）</p><ul><li>绕 x 轴</li></ul><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 &amp; 0\\0 &amp; \cos\theta &amp; -\sin\theta &amp; 0\\0 &amp; \sin\theta &amp; \cos\theta &amp; 0\\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p><ul><li><p>绕 y 轴</p><p><span class="math display">\[\left[\begin{array}{c}\cos\theta &amp; 0 &amp; \sin\theta &amp; 0\\0 &amp; 1 &amp; 0 &amp; 0\\-\sin\theta &amp; 0 &amp; \cos\theta &amp; 0\\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p></li><li><p>绕 z 轴 <span class="math display">\[\left[\begin{array}{c}\cos\theta &amp; -\sin\theta &amp; 0 &amp; 0\\\sin\theta &amp; \cos\theta &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]\]</span></p></li></ul></li></ol><hr><h4 id="d-变换之-mvp-矩阵">3D 变换之 MVP 矩阵</h4><p>MVP 矩阵，即Model(模型),View(视图),Projection(投影)三大变换矩阵，是将 3D模型转换为屏幕上的 2D 坐标的变换。</p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
      <tag>CG</tag>
      
      <tag>Games101</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>first-blog</title>
    <link href="/2024/09/10/first-blog/first-blog/"/>
    <url>/2024/09/10/first-blog/first-blog/</url>
    
    <content type="html"><![CDATA[<h1 id="my-first-blog-using-hexo">My First Blog Using Hexo!</h1>]]></content>
    
    
    
    <tags>
      
      <tag>test-blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
